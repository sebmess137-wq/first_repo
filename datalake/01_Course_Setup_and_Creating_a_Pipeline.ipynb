{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a3a0f9a-e481-4c53-a5c3-5d2a9905d249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JlQhiYprG9qk"
   },
   "source": [
    "# 1 - REQUIRED - Course Setup and Creating a Pipeline\n",
    "\n",
    "In this demo, we'll set up the course environment, explore its components, build a traditional ETL pipeline using JSON files as the data source, and then learn how to create a sample Lakeflow Spark Declarative Pipeline (SDP).\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Efficiently navigate the Workspace to locate course catalogs, schemas, and source files.\n",
    "- Create a Lakeflow Spark Declarative Pipeline using the Workspace and the Pipeline UI.\n",
    "\n",
    "\n",
    "### IMPORTANT - PLEASE READ!\n",
    "- **REQUIRED** - Run the **00_Setup_Environment.ipynb** notebook first to create the necessary catalog, schema, volume, and sample data files.\n",
    "- All tables will be created in the **ldp_demo.ldp_schema** catalog and schema.\n",
    "- All source data files are located in **/Volumes/ldp_demo/ldp_schema/raw/**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15dbb2bd-4aca-40e8-bba8-b848784ffe54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "A-DApRb3G9qo"
   },
   "source": [
    "## A. Environment Configuration\n",
    "\n",
    "Set up the environment variables for this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6059181b-36e0-48ac-b595-45ea93b12317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "D5fwejf-G9qp"
   },
   "outputs": [],
   "source": [
    "# Define catalog, schema, and volume paths\n",
    "CATALOG_NAME = 'cetpa_external_demo'\n",
    "SCHEMA_NAME = 'ldp_schema'\n",
    "VOLUME_NAME = 'raw'\n",
    "WORKING_DIR = f'/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}'\n",
    "\n",
    "print(f'Catalog: {CATALOG_NAME}')\n",
    "print(f'Schema: {SCHEMA_NAME}')\n",
    "print(f'Volume Path: {WORKING_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa412581-8843-4e20-8f93-76b8f8719ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0MnVFB76G9qs"
   },
   "source": [
    "## B. Explore the Lab Environment\n",
    "\n",
    "Explore the raw data source files, catalogs, and schema in the course lab environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "101dd83a-f05d-433c-899e-6466886e260c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wrDR_onTG9qs"
   },
   "source": [
    "1. Complete these steps to explore your catalog and schema you will be using in this course:\n",
    "\n",
    "   - a. Select the **Catalog** icon in the left navigation bar.\n",
    "\n",
    "   - b. You should see the **ldp_demo** catalog. You will use this catalog throughout the course.\n",
    "\n",
    "   - c. Expand your **ldp_demo** catalog. It should contain the **ldp_schema** schema where all tables will be created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dd23d6d-8cd5-41c6-bd7a-3d19271c1a54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "LdEFMOx-G9qt"
   },
   "source": [
    "2. Complete the following steps to view where our streaming raw source files are coming from:\n",
    "\n",
    "   a. Select the **Catalog** icon in the left navigation bar.\n",
    "\n",
    "   b. Expand the **ldp_demo** catalog.\n",
    "\n",
    "   c. Expand the **ldp_schema** schema and then **Volumes**.\n",
    "\n",
    "   d. Expand the **raw** volume. You should notice that your volume contains three folders:\n",
    "   - **customers**\n",
    "   - **orders**\n",
    "   - **status**\n",
    "\n",
    "   e. Expand each folder and notice that each cloud storage location contains a single JSON file to start with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37d56fcd-02ac-495b-ace3-88d41f444cb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "oYvkokdvG9qu"
   },
   "source": [
    "3. The volume path is `/Volumes/ldp_demo/ldp_schema/raw`. You can reference this path throughout the course using the `WORKING_DIR` variable defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42f159f-422b-47f7-b8ae-9bf10c363975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "v2LK2XwqG9qv"
   },
   "outputs": [],
   "source": [
    "print(f'Working directory: {WORKING_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20083560-31d5-485b-910a-75cf43cce592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "immdzjQ1G9qy"
   },
   "source": [
    "## C. Build a Traditional ETL Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63e6a6d4-179f-420c-af21-720fa3f3a413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "TvLU16zFG9qz"
   },
   "source": [
    "1. Query the raw JSON file(s) in your `/Volumes/ldp_demo/ldp_schema/raw/orders` volume to preview the data.\n",
    "\n",
    "      Notice that the JSON file is displayed ingested into tabular form using the `read_files` function. Take note of the following:\n",
    "\n",
    "    a. The **orders** JSON file contains order data for a company.\n",
    "\n",
    "    b. The one JSON file in your **/orders** volume (**00.json**) contains 174 rows. Remember that number for later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181ff787-63f0-4bf7-8653-8ce155056fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "eN6x8hRuG9q0"
   },
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "          SELECT *\n",
    "          FROM json.`/Volumes/cetpa_external_catalog/ldp_schema/raw/orders/`\n",
    "          ''').display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c1881e5-4f96-4130-9975-0c9fa2cdb612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "njlCkOXzG9q1"
   },
   "source": [
    "2. Traditionally, you would build an ETL pipeline by reading all of the files within the cloud storage location each time the pipeline runs. As data scales, this method becomes inefficient, more expensive, and time-consuming.\n",
    "\n",
    "   For example, you would write code like below.\n",
    "\n",
    "   **NOTES:**\n",
    "   - The tables and views will be written to your **ldp_demo.ldp_schema** schema.\n",
    "   - Knowledge of the Databricks `read_files` function is prerequisite for this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96647749-3a22-44ba-a8b8-a51e243d1b66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aHgTPHAKG9q1"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- JSON -> Bronze\n",
    "-- Read ALL files from your working directory each time the query is executed\n",
    "CREATE OR REPLACE TABLE cetpa_external_catalog.ldp_schema.orders_bronze\n",
    "AS\n",
    "SELECT\n",
    "  *,\n",
    "  current_timestamp() AS processing_time,\n",
    "  _metadata.file_name AS source_file\n",
    "FROM read_files(\n",
    "    '/Volumes/cetpa_external_catalog/ldp_schema/raw/orders',\n",
    "    format =>\"json\");\n",
    "\n",
    "\n",
    "-- Bronze -> Silver\n",
    "-- Read the entire bronze table each time the query is executed\n",
    "CREATE OR REPLACE TABLE cetpa_external_catalog.ldp_schema.orders_silver\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  timestamp(order_timestamp) AS order_timestamp,\n",
    "  customer_id,\n",
    "  notifications\n",
    "FROM cetpa_external_catalog.ldp_schema.orders_bronze;\n",
    "\n",
    "\n",
    "-- Silver -> Gold\n",
    "-- Aggregate the silver each time the query is executed.\n",
    "CREATE OR REPLACE VIEW cetpa_external_catalog.ldp_schema.orders_by_date_vw\n",
    "AS\n",
    "SELECT\n",
    "  date(order_timestamp) AS order_date,\n",
    "  count(*) AS total_daily_orders\n",
    "FROM cetpa_external_catalog.ldp_schema.orders_silver\n",
    "GROUP BY date(order_timestamp);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ce6d6a3-26e1-44ad-a6ce-a6e98c566996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "97lym2U-G9q1"
   },
   "source": [
    "3. Run the code in the cells to view the **orders_bronze** and **orders_silver** tables, and the **orders_by_date_vw** view. Explore the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec753e7-754f-47ab-a0f5-1a6ea86634ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DOy24syLG9q2"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM cetpa_external_catalog.ldp_schema.orders_bronze\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb84a82-4171-4ba6-8eff-4e81ee641842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NLqNzGWVG9q2"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM cetpa_external_catalog.ldp_schema.orders_silver\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff36c03-a54b-49ca-afd3-e9ffd952ead2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZmB4ONvJG9q2"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM cetpa_external_catalog.ldp_schema.orders_by_date_vw\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c913ebe-4e42-4069-ab8b-2d4f4cefa859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "x_tfmQqXG9q3"
   },
   "source": [
    "### Considerations\n",
    "\n",
    "- As JSON files are added to the volume in cloud storage, your **bronze table** code will read **all** of the files each time it executes, rather than reading only new rows of raw data. As the data grows, this can become inefficient and costly.\n",
    "\n",
    "- The **silver table** code will always read all the rows from the bronze table to prepare the silver table. As the data grows, this can also become inefficient and costly.\n",
    "\n",
    "- The traditional view, **orders_by_date_vw**, executes each time it is called. As the data grows, this can become inefficient.\n",
    "\n",
    "- To check data quality as new rows are added, additional code is needed to identify any values that do not meet the required conditions.\n",
    "\n",
    "- Monitoring the pipeline for each run is a challenge.\n",
    "\n",
    "- There is no simple user interface to explore, monitor, or fix issues everytime the code runs.\n",
    "\n",
    "### We can automatically process data incrementally, manage infrastructure, monitor, observe, optimize, and view this ETL pipeline by converting this to use **Spark Declarative Pipelines**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f54d6923-f71d-4a96-be02-385fec9f3271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "og1GVOzSG9q3"
   },
   "source": [
    "## D. Get Started Creating a Lakeflow Spark Declarative Pipeline Using the New Lakeflow Pipelines Editor\n",
    "\n",
    "In this section, we'll show you how to start creating a Spark Declarative Pipeline using the new Lakeflow Pipelines Editor. We won't run or modify the pipeline just yet!\n",
    "\n",
    "There are a few different ways to create your pipeline. Let's explore these methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d732186d-bca8-42b3-8af2-94a1253daa17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vXJK_aeuG9q3"
   },
   "source": [
    "1. First, complete the following steps to enable the new **Lakeflow Pipelines Editor**:\n",
    "\n",
    "   **NOTE:** This is being updated and how to enable it might change slightly moving forward.\n",
    "\n",
    "   a. In the top-right corner, select your user icon.\n",
    "\n",
    "   b. Right-click on **Settings** and select **Open in New Tab**.\n",
    "\n",
    "   c. Select **Developer**.\n",
    "\n",
    "   d. Scroll to the bottom and enable **Lakeflow Pipelines Editor** if it's not enabled and Click **Enable tabs for notebooks and files**.\n",
    "\n",
    "   e. Refresh your browser page to enable the option you turned on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5220b206-6a1a-4009-9efa-ad4aa6556c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Tg87_uXtG9q4"
   },
   "source": [
    "### D1. Create a Spark Declarative Pipeline Using the File Explorer\n",
    "1. Complete the following steps to create a Spark Declarative Pipeline using the left navigation pane:\n",
    "\n",
    "   a. In the left navigation bar, select the **Folder** icon to open the Workspace navigation.\n",
    "\n",
    "   b. Navigate to the **13 lakeflow declarative pipelines** folder.\n",
    "\n",
    "   c. (**PLEASE READ**) To complete this demonstration, it'll be easier to open this same notebook in another tab to follow along with these instructions. Right click on the notebook **01_Course_Setup_and_Creating_a_Pipeline** and select **Open in a New Tab**.\n",
    "\n",
    "   d. In the other tab select the three ellipsis icon in the folder navigation bar.\n",
    "\n",
    "   e. Select **Create** -> **ETL Pipeline**:\n",
    "      - If you have not enabled the new **Lakeflow Pipelines Editor** a pop-up might appear asking you to enable the new editor. Select **Enable** here or complete the previous step.\n",
    "\n",
    "      - Then use the following information:\n",
    "\n",
    "         - **Name**: `yourfirstname-my-pipeline-project`\n",
    "\n",
    "         - **Default catalog**: Select **ldp_demo** catalog\n",
    "\n",
    "         - **Default schema**: Select **ldp_schema** schema\n",
    "\n",
    "         - Select **Start with sample code in SQL**\n",
    "\n",
    "         The project will open up in the pipeline editor.\n",
    "\n",
    "   f. This will open your Spark Declarative Pipeline within the **Lakeflow Pipelines Editor**. By default, the project creates multiple folders and sample files for you as a starter. You can use this sample folder structure or create your own.\n",
    "\n",
    "   g. Close the link with the sample pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "768c8b02-5e6f-4e7f-9175-09004eb1c6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uu8ud2f1G9q4"
   },
   "source": [
    "### D2. Create a Spark Declarative Pipeline Using the Pipeline UI\n",
    "1. You can also create a Spark Declarative Pipeline using the far-left main navigation bar by completing the following steps:\n",
    "\n",
    "   a. On the far-left navigation bar, right-click **Jobs and Pipelines** and select **Open Link in New Tab**.\n",
    "\n",
    "   b. Find the blue **Create** button and select it.\n",
    "\n",
    "   c. Select **ETL pipeline**.\n",
    "\n",
    "   d. The same **Create pipeline** pop-up appears as before.\n",
    "\n",
    "   e. Here select **Add existing assets**.\n",
    "\n",
    "   f. The **Add existing assets** button enables you to select a folder with pipeline assets. This option will enable you to associate this new pipeline with code files already available in your Workspace, including Git folders.\n",
    "\n",
    "   g. You can close out of the pop up window and close the pipeline tab. You do not need to select a folder yet.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5061225292251803,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Course_Setup_and_Creating_a_Pipeline",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
