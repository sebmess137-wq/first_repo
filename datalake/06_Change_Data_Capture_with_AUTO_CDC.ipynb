{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee78490a-6318-4dda-821e-9e832f01744a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Hous-thNHAoV"
   },
   "source": [
    "# 6 - Change Data Capture with AUTO CDC with Slowing Changing Dimensions (SCD) TYPE 1\n",
    "\n",
    "##### NOTE: The AUTO CDC APIs replace the APPLY CHANGES APIs, and have the same syntax. The APPLY CHANGES APIs are still available, but Databricks recommends using the AUTO CDC APIs in their place.\n",
    "\n",
    "In this demonstration, we will continue to build our pipeline by ingesting **customer** data into our pipeline. The customer data includes new customers, customers who have deleted their accounts, and customers who have updated their information (such as address, email, etc.). We will need to build our customer pipeline by implementing change data capture (CDC) for customer data using SCD Type 1 (Type 2 is outside the scope of this course).\n",
    "\n",
    "The customer pipeline flow will:\n",
    "\n",
    "- The bronze table uses **Auto Loader** to ingest JSON data from cloud object storage with SQL (`FROM STREAM`).\n",
    "- A table is defined to enforce constraints before passing records to the silver layer.\n",
    "- `AUTO CDC` is used to automatically process CDC data into the silver layer as a Type 1.\n",
    "- A gold table is defined to create a materialized view of the current customers with updated information (dropped customers, new customers and updated customer information).\n",
    "\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, students should feel comfortable:\n",
    "- Apply the `AUTO CDC` operation in Lakeflow Spark Declarative Pipelines to process change data capture (CDC) by integrating and updating incoming data from a source stream into an existing Delta table, ensuring data accuracy and consistency.\n",
    "- Analyze Slowly Changing Dimensions (SCD Type 1) tables within Lakeflow Spark Declarative Pipelines to effectively update, insert and drop customers in dimensional data, managing the state of records over time using appropriate keys, versioning, and timestamps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ad069f9-9d7b-4c9d-9618-025bd1ee8e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fWhVZSccHAoZ"
   },
   "source": [
    "## A. Environment Configuration\n",
    "\n",
    "Set up the environment variables for this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f172e1ad-8b14-422e-b3b0-7d24b4a42313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4QGFyoRoHAoa"
   },
   "outputs": [],
   "source": [
    "# Define catalog, schema, and volume paths\n",
    "CATALOG_NAME = 'ldp_demo'\n",
    "SCHEMA_NAME = 'ldp_schema'\n",
    "VOLUME_NAME = 'raw'\n",
    "WORKING_DIR = f'/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}'\n",
    "\n",
    "print(f'Catalog: {CATALOG_NAME}')\n",
    "print(f'Schema: {SCHEMA_NAME}')\n",
    "print(f'Volume Path: {WORKING_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "406d1bc4-b788-4942-8d92-988920b95906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Qb6LtnQwHAod"
   },
   "source": [
    "## B. Explore the Customer Data Source Files\n",
    "\n",
    "1. Run the cell below to programmatically view the files in your `/Volumes/ldp_demo/ldp_schema/raw/customers` volume. Confirm you only see one **00.json** file for customers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "530f71f5-3ccd-4f15-b474-7094b0fe7f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1IGNKmxvHAoe"
   },
   "outputs": [],
   "source": [
    "spark.sql(f'LIST \"{WORKING_DIR}/customers\"').display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67c6d769-7d74-4788-8657-d1d9f3162fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UYiXH2bsHAof"
   },
   "source": [
    "2. Run the query below to explore the customers **00.json** file. Note the following:\n",
    "\n",
    "   a. The file contains customer information.\n",
    "\n",
    "   b. It includes general customer information such as **email**, **name**, and **address**.\n",
    "\n",
    "   c. The **timestamp** column specifies the logical order of customer events in the source data.\n",
    "\n",
    "   d. The **operation** column indicates whether the entry is for a new customer, a deletion, or an update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e54d3feb-7eb2-43e7-98cf-819c85cf13f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sX9N3ykIHAog"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/ldp_demo/ldp_schema/raw/customers/00.json',\n",
    "  format => \"JSON\"\n",
    ")\n",
    "ORDER BY operation;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e60112f-c4af-4c3a-8fe4-03b8e67d186f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6hzsAEnUHAoh"
   },
   "source": [
    "## C. Change Data Capture with AUTO CDC APIs in Lakeflow Spark Declarative Pipelines\n",
    "\n",
    "1. Create your starter Spark Declarative Pipeline for this demonstration. The pipeline should be configured with:\n",
    "    - Your default catalog: `ldp_demo`\n",
    "    - Your default schema: `ldp_schema`\n",
    "    - Your configuration parameter: `source` = `/Volumes/ldp_demo/ldp_schema/raw`\n",
    "    - Source folders: `orders`, `status`, `customers`\n",
    "\n",
    "2. Complete the following steps to open the starter Spark Declarative Pipeline project:\n",
    "\n",
    "   a. In the main navigation bar right-click on **Jobs & Pipelines** and select **Open in Link in New Tab**.\n",
    "\n",
    "   b. In **Jobs & Pipelines** select your pipeline (or create a new one).\n",
    "\n",
    "   c. **REQUIRED:** At the top near your pipeline name, turn on **New pipeline monitoring**.\n",
    "\n",
    "   d. In the **Pipeline details** pane on the far right, select **Open in Editor** to open the pipeline in the **Lakeflow Pipeline Editor**.\n",
    "\n",
    "   e. In the new tab you should see folders: **orders**, **status**, and **customers**.\n",
    "\n",
    "3. Explore the code in the `customers/customers_pipeline.sql` file. Follow the instructional comments in the file to proceed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5ce3539-e58f-4220-982a-9465f1b24a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5DPBduMyHAoi"
   },
   "source": [
    "## D. Land New Data to Your Data Source Volume\n",
    "\n",
    "1. Run the cell below to land a new JSON file to each volume (**customers**, **status** and **orders**) to simulate new files being added to your cloud storage locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe0aeac4-6a81-421b-a106-3b634c599d2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "u6VJ2B-YHAoj"
   },
   "outputs": [],
   "source": [
    "def copy_files(copy_from: str, copy_to: str, n: int, sleep=2):\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    print(f\"\\n----------------Loading files to volume: '{copy_to}'----------------\")\n",
    "\n",
    "    if os.path.exists(copy_from):\n",
    "        list_of_files_to_copy = sorted(os.listdir(copy_from))\n",
    "        total_files = len(list_of_files_to_copy)\n",
    "    else:\n",
    "        print(f'Source directory {copy_from} does not exist.')\n",
    "        return\n",
    "\n",
    "    if os.path.exists(copy_to):\n",
    "        list_of_files_in_dest = os.listdir(copy_to)\n",
    "    else:\n",
    "        list_of_files_in_dest = []\n",
    "\n",
    "    assert total_files >= n, f\"Source location contains only {total_files} files, but you specified {n} files to copy.\"\n",
    "\n",
    "    counter = 1\n",
    "    for file in list_of_files_to_copy:\n",
    "        if file in list_of_files_in_dest:\n",
    "            print(f'File number {counter} - {file} already exists. Skipping.')\n",
    "        else:\n",
    "            file_to_copy = f'{copy_from}/{file}'\n",
    "            copy_file_to = f'{copy_to}/{file}'\n",
    "            print(f'File number {counter} - Copying {file_to_copy} --> {copy_file_to}')\n",
    "            dbutils.fs.cp(file_to_copy, copy_file_to, recurse=True)\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        if counter == n:\n",
    "            break\n",
    "        counter += 1\n",
    "\n",
    "# Copy files for multiple sources\n",
    "try:\n",
    "    copy_files(\n",
    "        copy_from='/Volumes/dbacademy_retail/v01/retail-pipeline/customers/stream_json',\n",
    "        copy_to=f'{WORKING_DIR}/customers',\n",
    "        n=2,\n",
    "        sleep=1\n",
    "    )\n",
    "    copy_files(\n",
    "        copy_from='/Volumes/dbacademy_retail/v01/retail-pipeline/orders/stream_json',\n",
    "        copy_to=f'{WORKING_DIR}/orders',\n",
    "        n=2,\n",
    "        sleep=1\n",
    "    )\n",
    "    copy_files(\n",
    "        copy_from='/Volumes/dbacademy_retail/v01/retail-pipeline/status/stream_json',\n",
    "        copy_to=f'{WORKING_DIR}/status',\n",
    "        n=2,\n",
    "        sleep=1\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'Note: Could not copy from dbacademy_retail. Error: {e}')\n",
    "    print(f'Please manually add JSON files to the volumes.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d96b2db1-3cae-4505-8f32-5e5d728c75ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "y0u9dgN-HAok"
   },
   "source": [
    "2. Run the cell below to programmatically view the files in your `/Volumes/ldp_demo/ldp_schema/raw/customers` volume. Confirm your volume now contains the original **00.json** file and the new **01.json** file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6910a61e-49c0-4557-b1c2-605158a037e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hp1y1gHaHAol"
   },
   "outputs": [],
   "source": [
    "spark.sql(f'LIST \"{WORKING_DIR}/customers\"').display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba2ced8b-3ccd-4f62-a193-331e85f1ccc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5mektE0nHAol"
   },
   "source": [
    "3. Go back to your pipeline and click **Run pipeline** button to ingest the new JSON file (**01.json**) incrementally and perform CDC SCD Type 1 on the **scd_type_1_customers_silver_demo6** table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "547cd7f0-f574-49c1-9efc-01ff81094d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9aVvwR-1HAom"
   },
   "source": [
    "## E. Explore the Customers Pipeline Tables\n",
    "\n",
    "1. Run the query below to view the **scd_type_1_customers_silver_demo6** streaming table (the table with SCD Type 1 updates, inserts and deletes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18f9fb33-296a-4628-af64-83d15804b391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "oDwthGSWHAom"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM ldp_demo.ldp_schema.scd_type_1_customers_silver_demo6;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_Change_Data_Capture_with_AUTO_CDC",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
