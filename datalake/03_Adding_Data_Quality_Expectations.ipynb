{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2168b2b3-628e-4ce5-b655-1a5d3a9eaefb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "lVUfriHbG-mt"
   },
   "source": [
    "# 3 - Adding Data Quality Expectations\n",
    "\n",
    "In this demonstration we will add data quality expectations to apply quality constraints that validates data as it flows through Lakeflow Spark Declarative Pipelines. Expectations provide greater insight into data quality metrics and allow you to fail updates or drop records when detecting invalid records.\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Add quality constraints within a Lakeflow Spark Declarative Pipeline to trigger appropriate actions (warn, drop, or fail) based on data expectations.\n",
    "- Analyze pipeline metrics to identify and interpret data quality issues across different data flows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f75efac1-52b6-4a67-91c3-cca2f90c4fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6lAIKiroG-mx"
   },
   "source": [
    "## A. Environment Configuration\n",
    "\n",
    "Set up the environment variables for this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcdb04a9-b8c0-4ae9-8270-dd4f7f1d1feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vHKDjFBDG-my"
   },
   "outputs": [],
   "source": [
    "# Define catalog, schema, and volume paths\n",
    "CATALOG_NAME = 'cetpa_external_catalog'\n",
    "SCHEMA_NAME = 'ldp_schema'\n",
    "VOLUME_NAME = 'raw'\n",
    "WORKING_DIR = f'/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}'\n",
    "\n",
    "print(f'Catalog: {CATALOG_NAME}')\n",
    "print(f'Schema: {SCHEMA_NAME}')\n",
    "print(f'Volume Path: {WORKING_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7644bb58-04b8-437e-99ca-ff5423995052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4tUZtSdHG-mz"
   },
   "source": [
    "Run the cell below to programmatically view the files in your `/Volumes/ldp_demo/ldp_schema/raw/orders` volume. Confirm you only see the original **00.json** file in the **orders** folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eedcc3f4-574c-44b1-abbb-9b2487129124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "X8WrFJ8GG-m0"
   },
   "outputs": [],
   "source": [
    "spark.sql(f'LIST \"{WORKING_DIR}/orders\"').display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "398788bd-1d41-4437-8712-a3e57b5e1b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Zrw4oSVXG-m1"
   },
   "source": [
    "## B. Adding Data Quality Expectations\n",
    "\n",
    "This demonstration includes the simple starter Spark Declarative Pipeline that has already been created in the previous demonstration. We will continue to build on it to explore it's capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a5d76ff-55d3-4ffd-a398-893bfe158837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xInYBj8ZG-m2"
   },
   "source": [
    "1. Create your starter pipeline for this demonstration. The pipeline should be configured with:\n",
    "\n",
    "- Your default catalog: `ldp_demo`\n",
    "- Your default schema: `ldp_schema`\n",
    "- Your configuration parameter: `source` = `/Volumes/ldp_demo/ldp_schema/raw`\n",
    "\n",
    "  **NOTE:** If the pipeline already exists, you'll need to delete the existing pipeline first.\n",
    "\n",
    "  To delete the pipeline:\n",
    "\n",
    "  - Select **Jobs and Pipelines** from the far-left navigation bar.  \n",
    "\n",
    "  - Find the pipeline you want to delete.  \n",
    "\n",
    "  - Click the three-dot menu.  \n",
    "\n",
    "  - Select **Delete**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c13728-93c9-4427-8ec8-a3b30f4f634a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ucSLBdyaG-m2"
   },
   "source": [
    "2. Complete the following steps to open the starter Spark Declarative Pipeline project for this demonstration:\n",
    "\n",
    "   a. In the main navigation bar right-click on **Jobs & Pipelines** and select **Open in Link in New Tab**.\n",
    "\n",
    "   b. In **Jobs & Pipelines** select your pipeline (or create a new one).\n",
    "\n",
    "   c. **REQUIRED:** At the top near your pipeline name, turn on **New pipeline monitoring**.\n",
    "\n",
    "   d. In the **Pipeline details** pane on the far right, select **Open in Editor** (field to the right of **Source code**) to open the pipeline in the **Lakeflow Pipeline Editor**.\n",
    "\n",
    "   e. In the new tab:\n",
    "      - Select the **orders** folder\n",
    "\n",
    "      - Click on **orders_pipeline.sql**.\n",
    "\n",
    "   f. In the navigation pane of the new tab, you should see **Pipeline** and **All Files**. Ensure you are in the **Pipeline** tab. This will list all files in your pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45d2b596-cfbb-4f75-a4f0-d6e7e9dbf81c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mBNppxLUG-m3"
   },
   "source": [
    "3. In the new tab, follow the instructions provided in the comments within the **orders_pipeline.sql** file to add data quality expectations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61566741-dc06-49bc-8e94-934efdb97bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tdKe0A1gG-m4"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Manage data quality with pipeline expectations](https://docs.databricks.com/aws/en/dlt/expectations)\n",
    "\n",
    "- [Expectation recommendations and advanced patterns](https://docs.databricks.com/aws/en/dlt/expectation-patterns)\n",
    "\n",
    "- [Data Quality Management With Databricks](https://www.databricks.com/discover/pages/data-quality-management#expectations-with-delta-live-tables)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Adding_Data_Quality_Expectations",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
