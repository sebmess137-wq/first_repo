{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75ce8968-6eb8-4176-975e-97f0f0185cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Wbhgm0b2G-St"
   },
   "source": [
    "# 2 - Developing a Simple Pipeline\n",
    "\n",
    "In this demonstration, we will create a simple Lakeflow Spark Declarative Pipeline project using the new **Lakeflow Pipeline Editor** with declarative SQL.\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Describe the SQL syntax used to create a Lakeflow Spark Declarative Pipeline.\n",
    "- Navigate the Lakeflow Pipeline Editor to modify pipeline settings and ingest the raw data source file(s).\n",
    "- Create, execute and monitor a Spark Declarative Pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8502b9a4-ee8f-46bb-b98b-f95659e2db6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "u16DcxprG-S1"
   },
   "source": [
    "## A. Environment Configuration\n",
    "\n",
    "Set up the environment variables for this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1a1f68-418e-4ed3-afee-d3776397cd5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-oTA--JJG-S3"
   },
   "outputs": [],
   "source": [
    "# Define catalog, schema, and volume paths\n",
    "CATALOG_NAME = 'cetpa_external_catalog'\n",
    "SCHEMA_NAME = 'ldp_schema'\n",
    "VOLUME_NAME = 'raw'\n",
    "WORKING_DIR = f'/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}'\n",
    "\n",
    "print(f'Catalog: {CATALOG_NAME}')\n",
    "print(f'Schema: {SCHEMA_NAME}')\n",
    "print(f'Volume Path: {WORKING_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c8f5b84-c7ab-46eb-a50e-3f0de9aea863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "LhB8GEC_G-S5"
   },
   "source": [
    "## B. Developing and Running a Spark Declarative Pipeline with the Lakeflow Pipeline Editor\n",
    "\n",
    "This course includes a simple, pre-configured Spark Declarative Pipeline to explore and modify.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "- Explore the Lakeflow Pipeline Editor and the declarative SQL syntax  \n",
    "- Modify pipeline settings  \n",
    "- Run the Spark Declarative Pipeline and explore the streaming tables and materialized view.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11a2d55b-b35e-4a1b-9289-47cdaf060759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qKXcCxZkG-S_"
   },
   "source": [
    "1. The volume path is `/Volumes/ldp_demo/ldp_schema/raw`. You will need this path when modifying your pipeline settings.\n",
    "\n",
    "   This volume path contains the **orders**, **status** and **customer** directories, which contain the raw JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "824913ff-9188-47e8-bdb7-72ac2ec72994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "TLiWjkNgG-TG"
   },
   "outputs": [],
   "source": [
    "print(f'Working directory: {WORKING_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b775ca4d-3a63-472d-b11e-c62fde1646bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KYK9kee-G-TH"
   },
   "source": [
    "2. In this course we have starter files for you to use in your pipeline. This demonstration uses the folder **2 - Developing a Simple Pipeline Project**. To create a pipeline and add existing assets to associate it with code files already available in your Workspace (including Git folders) complete the following:\n",
    "\n",
    "   a. For ease of use, open **Jobs & Pipelines** in a separate tab:\n",
    "\n",
    "    - On the main navigation bar, right-click on **Jobs & Pipelines** and select **Open in a New Tab**.\n",
    "\n",
    "   b. In **Jobs & Pipelines** select **Create** â†’ **ETL Pipeline**.\n",
    "\n",
    "   c. Complete the pipeline creation page with the following:\n",
    "\n",
    "    - **Name**: `Name-your-pipeline-using-this-notebook-name-add-your-first-name`\n",
    "    - **Default catalog**: Select **ldp_demo** catalog  \n",
    "    - **Default schema**: Select **ldp_schema** schema\n",
    "    - Notice there are a variety of options to start your pipeline.\n",
    "\n",
    "   d. In the options, select **Add existing assets**. In the popup, complete the following:\n",
    "\n",
    "    - **Pipeline root folder**: Select the **2 - Developing a Simple Pipeline Project** folder\n",
    "\n",
    "    - **Source code paths**: Within the same root folder as above, select the **orders** folder\n",
    "\n",
    "    **NOTE:** You can select folders containing SQL and Python files to be executed as part of the pipeline, or you can provide individual file paths. The specified files will be processed when the pipeline runs.\n",
    "\n",
    "   e. Click **Add**, This will create a pipeline and associate the correct files for this demonstration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a71af91-2f7f-4ed9-a00e-321ac5c46c73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "lhW_GDk3G-TX"
   },
   "source": [
    "3. In the new window, select the **orders_pipeline.sql** file and follow the instructions in the SQL file within the **Lakeflow Pipelines Editor**.\n",
    "\n",
    "    Leave this notebook open as you will use it later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fc21110-c022-44ed-87cb-09c0eb15fa37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "t2hEvPlRG-Tg"
   },
   "source": [
    "## C. Add a New File to Cloud Storage\n",
    "\n",
    "1. After exploring and executing the pipeline by following the instructions in the **`orders_pipeline.sql`** file, run the cell below to add a new JSON file (**01.json**) to your volume at: `/Volumes/ldp_demo/ldp_schema/raw/orders`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108b129c-8121-4b3e-aa97-c185eb20ca1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "W8JIxtn8G-Ti"
   },
   "outputs": [],
   "source": [
    "def copy_files(copy_from: str, copy_to: str, n: int, sleep=2):\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    print(f\"\\n----------------Loading files to volume: '{copy_to}'----------------\")\n",
    "\n",
    "    if os.path.exists(copy_from):\n",
    "        list_of_files_to_copy = sorted(os.listdir(copy_from))\n",
    "        total_files = len(list_of_files_to_copy)\n",
    "    else:\n",
    "        print(f'Source directory {copy_from} does not exist.')\n",
    "        return\n",
    "\n",
    "    if os.path.exists(copy_to):\n",
    "        list_of_files_in_dest = os.listdir(copy_to)\n",
    "    else:\n",
    "        list_of_files_in_dest = []\n",
    "\n",
    "    assert total_files >= n, f\"Source location contains only {total_files} files, but you specified {n} files to copy.\"\n",
    "\n",
    "    counter = 1\n",
    "    for file in list_of_files_to_copy:\n",
    "        if file in list_of_files_in_dest:\n",
    "            print(f'File number {counter} - {file} already exists. Skipping.')\n",
    "        else:\n",
    "            file_to_copy = f'{copy_from}/{file}'\n",
    "            copy_file_to = f'{copy_to}/{file}'\n",
    "            print(f'File number {counter} - Copying {file_to_copy} --> {copy_file_to}')\n",
    "            dbutils.fs.cp(file_to_copy, copy_file_to, recurse=True)\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        if counter == n:\n",
    "            break\n",
    "        counter += 1\n",
    "\n",
    "# Copy additional orders file\n",
    "try:\n",
    "    copy_files(\n",
    "        copy_from='/Volumes/retail/v01/retail-pipeline/orders/stream_json',\n",
    "        copy_to=f'{WORKING_DIR}/orders',\n",
    "        n=2\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'Note: Could not copy from retail. Error: {e}')\n",
    "    print(f'Please manually add JSON files to: {WORKING_DIR}/orders/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0df8015-1d7c-4774-a47d-fc7de223e3c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mZemw16vG-Tk"
   },
   "source": [
    "2. Complete the following steps to view the new file in your volume:\n",
    "\n",
    "   a. Select the **Catalog** icon from the left navigation pane.  \n",
    "\n",
    "   b. Expand your **ldp_demo.ldp_schema.raw** volume.  \n",
    "\n",
    "   c. Expand the **orders** directory. You should see two files in your volume: **00.json** and **01.json**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "494fee4c-3910-40be-8d09-8217be695d38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "O72kpDXcG-Tl"
   },
   "source": [
    "3. Run the cell below to view the data in the new **/orders/01.json** file. Notice the following:\n",
    "\n",
    "   - The **01.json** file contains new orders.  \n",
    "   - The **01.json** file has 25 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0be31d-5d78-488b-9dda-85fa32b16acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "48MHML0lG-Tm"
   },
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "  SELECT *\n",
    "  FROM json.`/Volumes/cetpa_external_catalog/ldp_schema/raw/orders/00.json`\n",
    "''').display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d83592b4-3f07-4e48-b4b2-2893b9dfa2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mjbjH4ZJG-To"
   },
   "source": [
    "4. Go back to the **orders_pipeline.sql** file and select **Run Pipeline** to execute your ETL pipeline again with the new file.  \n",
    "\n",
    "   Watch the pipeline run and notice only 25 rows are added to the bronze and silver tables.\n",
    "\n",
    "   This happens because the pipeline has already processed the first **00.json** file (174 rows), and it is now only reading the new **01.json** file (25 rows), appending the rows to the streaming tables, and recomputing the materialized view with the latest data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbebee84-c265-4197-9e20-caf70a0e1471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "h-NvZ6aIG-To"
   },
   "source": [
    "## D. Exploring Your Streaming Tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "150a4eef-c548-442d-ab84-51505dae8162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DT_qsmjSG-Tp"
   },
   "source": [
    "1. View the new streaming tables and materialized view in your catalog. Complete the following:\n",
    "\n",
    "   a. Select the catalog icon in the left navigation pane.\n",
    "\n",
    "   b. Expand your **ldp_demo** catalog.\n",
    "\n",
    "   c. Expand the **ldp_schema** schema. Notice that the streaming tables and materialized view are correctly placed in your schema.\n",
    "\n",
    "      - **ldp_demo.ldp_schema.orders_bronze_demo2**\n",
    "\n",
    "      - **ldp_demo.ldp_schema.orders_silver_demo2**\n",
    "\n",
    "      - **ldp_demo.ldp_schema.orders_by_date_gold_demo2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "952ccffa-cd74-469e-8b67-65eb20adeb9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "VfoqhcI1G-Tq"
   },
   "source": [
    "2. Run the cell below to view the data in the **ldp_demo.ldp_schema.orders_bronze_demo2** table. Before you run the cell, how many rows should this streaming table have?\n",
    "\n",
    "   Notice the following:\n",
    "      - The table contains 199 rows (**00.json** had 174 rows, and **01.json** had 25 rows).\n",
    "      - In the **source_file** column you can see the exact file the rows were ingested from.\n",
    "      - In the **processing_time** column you can see the exact time the rows were ingested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a9aad5-7a38-44b5-8853-962c87e6075c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "f085cUH_G-Tr"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM cetpa_external_catalog.ldp_schema.orders_bronze_demo2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7a723c2-3fa0-4dc6-83f7-2acd28c350b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "jPWVv_TkG-Ts"
   },
   "source": [
    "## E. Viewing Spark Declarative Pipelines with the Pipelines UI\n",
    "\n",
    "After exploring and creating your pipeline using the **orders_pipeline.sql** file in the steps above, you can view the pipeline(s) you created in your workspace via the **Jobs and Pipelines** UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1ec522-1805-4fdb-b2fd-7fcdb1b92ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "b7zQ6QPXG-Ts"
   },
   "source": [
    "1. Complete the following steps to view the pipeline you created:\n",
    "\n",
    "   a. In the main applications navigation pane on the far left, right-click on **Jobs & Pipelines** and select **Open Link in a New Tab**.\n",
    "\n",
    "   b. This should take you to the pipelines you have created. You should see your pipeline.\n",
    "\n",
    "   c. Select your pipeline. Here, you can use the UI to modify the pipeline.\n",
    "\n",
    "   d. Select the **Settings** button at the top. This will take you to the settings within the UI.\n",
    "\n",
    "   e. Select **Schedule** to schedule the pipeline. Select **Cancel**, we will learn how to schedule the pipeline later.\n",
    "\n",
    "   f. Under your pipeline name, select the drop-down with the time date stamp. Here you can view the **Pipeline graph** and other metrics for each run of the pipeline.\n",
    "\n",
    "   g. Close the pipeline UI tab you opened.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ab5b88c-8557-4aac-998a-71a2d835d73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hGBtd0P_G-Tt"
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Lakeflow Spark Declarative Pipelines](https://docs.databricks.com/aws/en/dlt/) documentation.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5061225292251832,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Developing_a_Simple_Pipeline",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
