{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab56bb89-d93e-44ab-ab33-d56e8e5390de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2YWJXNCLHAIh"
   },
   "source": [
    "# 5 - Deploying a Pipeline to Production\n",
    "\n",
    "In this demonstration, we will begin by adding an additional data source to our pipeline and performing a join with our streaming tables. Then, we will focus on productionalizing the pipeline by adding comments and table properties to the objects we create, scheduling the pipeline, and creating an event log to monitor the pipeline.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Apply the appropriate comment syntax and table properties to pipeline objects to enhance readability.\n",
    "- Demonstrate how to perform a join between two streaming tables using a materialized view to optimize data processing.\n",
    "- Execute the scheduling of a pipeline using trigger or continuous modes to ensure timely processing.\n",
    "- Explore the event log to monitor a production Lakeflow Spark Declarative Pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c28fbadb-89a3-440f-a1e8-74feaa66537b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HyyzkP_IHAIm"
   },
   "source": [
    "## A. Environment Configuration\n",
    "\n",
    "Set up the environment variables for this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9059a249-372d-4ec9-9dfc-61cb8360b068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "l-6pbNGXHAIo"
   },
   "outputs": [],
   "source": [
    "# Define catalog, schema, and volume paths\n",
    "CATALOG_NAME = 'ldp_demo'\n",
    "SCHEMA_NAME = 'ldp_schema'\n",
    "VOLUME_NAME = 'raw'\n",
    "WORKING_DIR = f'/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}'\n",
    "\n",
    "print(f'Catalog: {CATALOG_NAME}')\n",
    "print(f'Schema: {SCHEMA_NAME}')\n",
    "print(f'Volume Path: {WORKING_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a1244aa-1fe0-4eb6-a5b0-35755c21ff65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hfYYJxfDHAIr"
   },
   "source": [
    "## B. Explore the Orders and Status JSON Files\n",
    "\n",
    "1. Explore the raw data located in the `/Volumes/ldp_demo/ldp_schema/raw/orders/` volume. This is the data we have been working with throughout the course demonstrations.\n",
    "\n",
    "   Run the cell below to view the results. Notice that the orders JSON file(s) contains information about when each order was placed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "678541e9-20d0-42c6-a970-49b80542f2ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "YAH8ieckHAIt"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/ldp_demo/ldp_schema/raw/orders/',\n",
    "  format => 'JSON'\n",
    ")\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1ae07fa-df2b-432e-ab28-ba5debd72386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "I7cvErNsHAIv"
   },
   "source": [
    "2. Explore the **status** raw data located in the `/Volumes/ldp_demo/ldp_schema/raw/status/` volume and filter for the specific **order_id** *75123*.\n",
    "\n",
    "   Run the cell below to view the results. Notice that the status JSON file(s) contain **order_status** information for each order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "805bb972-80ce-4194-8070-8346968c075b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "seCvVLtcHAIw"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/ldp_demo/ldp_schema/raw/status/',\n",
    "  format => 'JSON'\n",
    ")\n",
    "WHERE order_id = 75123;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34c0cccc-5c05-40fc-b880-ba38b8e84fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "g9timTDAHAIz"
   },
   "source": [
    "## C. Create Production Pipeline\n",
    "\n",
    "1. Create your starter pipeline for this demonstration. The pipeline should be configured with:\n",
    "\n",
    "- Your default catalog: `ldp_demo`\n",
    "- Your default schema: `ldp_schema`\n",
    "- Your configuration parameter: `source` = `/Volumes/ldp_demo/ldp_schema/raw`\n",
    "\n",
    "2. Complete the following steps to open the starter Spark Declarative Pipeline project:\n",
    "\n",
    "   a. In the main navigation bar right-click on **Jobs & Pipelines** and select **Open in Link in New Tab**.\n",
    "\n",
    "   b. In **Jobs & Pipelines** select your pipeline (or create a new one).\n",
    "\n",
    "   c. **REQUIRED:** At the top near your pipeline name, turn on **New pipeline monitoring**.\n",
    "\n",
    "   d. In the **Pipeline details** pane on the far right, select **Open in Editor** to open the pipeline in the **Lakeflow Pipeline Editor**.\n",
    "\n",
    "   e. In the new tab you should see folders: **orders** and **status**.\n",
    "\n",
    "3. Explore the code in the `orders/orders_pipeline.sql` file and `status/status_pipeline.sql` file. Follow the instructional comments in the files to proceed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0726a4db-8327-439d-ad6b-5fa8aa244ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hpVcS127HAI1"
   },
   "source": [
    "## D. Land More Data to Your Data Source Volume\n",
    "\n",
    "1. Run the cell below to add more JSON files to your volumes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5d6704e-9a3a-4253-aab5-bfc54c994e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sJRMy9SeHAI3"
   },
   "outputs": [],
   "source": [
    "def copy_files(copy_from: str, copy_to: str, n: int, sleep=2):\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    print(f\"\\n----------------Loading files to volume: '{copy_to}'----------------\")\n",
    "\n",
    "    if os.path.exists(copy_from):\n",
    "        list_of_files_to_copy = sorted(os.listdir(copy_from))\n",
    "        total_files = len(list_of_files_to_copy)\n",
    "    else:\n",
    "        print(f'Source directory {copy_from} does not exist.')\n",
    "        return\n",
    "\n",
    "    if os.path.exists(copy_to):\n",
    "        list_of_files_in_dest = os.listdir(copy_to)\n",
    "    else:\n",
    "        list_of_files_in_dest = []\n",
    "\n",
    "    assert total_files >= n, f\"Source location contains only {total_files} files, but you specified {n} files to copy.\"\n",
    "\n",
    "    counter = 1\n",
    "    for file in list_of_files_to_copy:\n",
    "        if file in list_of_files_in_dest:\n",
    "            print(f'File number {counter} - {file} already exists. Skipping.')\n",
    "        else:\n",
    "            file_to_copy = f'{copy_from}/{file}'\n",
    "            copy_file_to = f'{copy_to}/{file}'\n",
    "            print(f'File number {counter} - Copying {file_to_copy} --> {copy_file_to}')\n",
    "            dbutils.fs.cp(file_to_copy, copy_file_to, recurse=True)\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        if counter == n:\n",
    "            break\n",
    "        counter += 1\n",
    "\n",
    "# Copy additional files\n",
    "try:\n",
    "    copy_files(\n",
    "        copy_from='/Volumes/dbacademy_retail/v01/retail-pipeline/orders/stream_json',\n",
    "        copy_to=f'{WORKING_DIR}/orders',\n",
    "        n=5\n",
    "    )\n",
    "\n",
    "    copy_files(\n",
    "        copy_from='/Volumes/dbacademy_retail/v01/retail-pipeline/status/stream_json',\n",
    "        copy_to=f'{WORKING_DIR}/status',\n",
    "        n=5\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'Note: Could not copy from dbacademy_retail. Error: {e}')\n",
    "    print(f'Please manually add JSON files to: {WORKING_DIR}/orders/ and {WORKING_DIR}/status/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80c58784-0069-40f0-a6a5-49f3db16bdb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "PhHe5yGVHAI_"
   },
   "source": [
    "2. Navigate back to your pipeline and select **Run pipeline** to process the new landed files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c59b478-865c-412a-8a37-07a1b2f62ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FnokJdRTHAJA"
   },
   "source": [
    "## E. Monitor Your Pipeline with the Event Log\n",
    "\n",
    "After running your pipeline and successfully publishing the event log, you can explore the event log. The event log provides detailed information about pipeline runs, data quality metrics, and other pipeline events.\n",
    "\n",
    "1. Query your event log table (if configured) to see pipeline events and metrics.\n",
    "\n",
    "2. Explore data quality metrics and pipeline performance through the event log.\n",
    "\n",
    "For more information, see the [Monitor Lakeflow Spark Declarative Pipelines](https://docs.databricks.com/aws/en/dlt/observability) documentation.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Deploying_a_Pipeline_to_Production",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
