{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ddbb3dc-d1f1-4b28-af37-cccc20a4f2e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "T-95zZYGG_jO"
   },
   "source": [
    "# 4 Lab - Create a Pipeline  \n",
    "\n",
    "In this lab, you'll migrate a traditional ETL workflow to a pipeline for incremental data processing. You'll practice building streaming tables and materialized views using Lakeflow Spark Declarative Pipelines syntax.\n",
    "\n",
    "#### Your Tasks:\n",
    "- Create a new Pipeline  \n",
    "- Convert traditional SQL ETL to declarative syntax for incremental processing\n",
    "- Configure pipeline settings  \n",
    "- Define data quality expectations  \n",
    "- Validate and run the pipeline\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Create a pipeline and execute it successfully using the new Lakeflow Pipeline Editor.\n",
    "- Modify and configure pipeline settings to align with specific data processing requirements.\n",
    "- Integrate data quality expectations into a pipeline and evaluate their effectiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2162f340-07fa-462e-901e-314dc26cd002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WQ5m-vjiG_jS"
   },
   "source": [
    "## A. Environment Configuration\n",
    "\n",
    "Set up the environment variables for this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "814f4ddd-361e-4fe9-9edf-66f397a41b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2ZzAhkyLG_jW"
   },
   "outputs": [],
   "source": [
    "# Define catalog, schema, and volume paths\n",
    "CATALOG_NAME = 'ldp_demo'\n",
    "SCHEMA_NAME = 'ldp_schema'\n",
    "VOLUME_NAME = 'raw'\n",
    "WORKING_DIR = f'/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}'\n",
    "\n",
    "print(f'Catalog: {CATALOG_NAME}')\n",
    "print(f'Schema: {SCHEMA_NAME}')\n",
    "print(f'Volume Path: {WORKING_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "596a9258-487f-4fde-8ccf-f71f79ac9c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HSkpFORbG_ja"
   },
   "source": [
    "## B. SCENARIO\n",
    "\n",
    "Your data engineering team has identified an opportunity to modernize an existing ETL pipeline that was originally developed in a Databricks notebook. While the current pipeline gets the job done, it lacks the scalability, observability, efficiency and automated data quality features required as your data volume and complexity grow.\n",
    "\n",
    "To address this, you've been asked to migrate the existing pipeline to a Lakeflow Spark Declarative Pipeline. Spark Declarative Pipelines will enable your team to define data transformations more declaratively, apply data quality rules, and benefit from built-in optimization, lineage tracking and monitoring.\n",
    "\n",
    "Your goal is to refactor the original notebook based logic (shown in the cells below) into a Spark Declarative Pipeline.\n",
    "\n",
    "### REQUIREMENTS:\n",
    "  - Migrate the ETL code below to a Spark Declarative Pipeline.\n",
    "  - Add the required data quality expectations to the bronze table and silver table.\n",
    "  - Create materialized views for the most up to date aggregated information.\n",
    "\n",
    "Follow the steps below to complete your task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcd3d9f2-b14f-40bf-8609-0e9fcc960765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dsULsL2CG_je"
   },
   "source": [
    "### B1. Explore the Raw Data\n",
    "\n",
    "1. Complete the following steps to view where our lab's streaming raw source files are coming from:\n",
    "\n",
    "   a. Select the **Catalog** icon in the left navigation bar.  \n",
    "\n",
    "   b. Expand your **ldp_demo** catalog.  \n",
    "\n",
    "   c. Expand the **ldp_schema** schema.  \n",
    "\n",
    "   d. Expand **Volumes**.  \n",
    "\n",
    "   e. Expand the **raw** volume.  \n",
    "\n",
    "   f. You should see folders: **customers**, **orders**, and **status**.  \n",
    "\n",
    "   g. The files in the **raw** volume will be the data source files you will be ingesting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f680345-0a8f-4ea3-a868-85ccda59678d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zEyB1o5tG_jg"
   },
   "source": [
    "### B2. Current ETL Code\n",
    "\n",
    "Run each cell below to view the results of the current ETL pipeline. This will give you an idea of the expected output. Don't worry too much about the data transformations within the SQL queries.\n",
    "\n",
    "The focus of this lab is on using **declarative SQL** and creating a **Spark Declarative Pipeline**. You will not need to modify the transformation logic, only the `CREATE` statements and `FROM` clauses to ensure data is read and processed incrementally in your pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb3ef603-dcc0-4289-98db-fc1196419a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mzIk52chG_jg"
   },
   "source": [
    "#### B2.1 - JSON to Bronze\n",
    "\n",
    "Explore the code and run the cell. Observe the results. Notice that:\n",
    "\n",
    "- The JSON file in the volume is read in as a table named **orders_bronze_lab4** in the **ldp_demo.ldp_schema** schema.  \n",
    "\n",
    "Think about what you will need to change when migrating this to a Spark Declarative Pipeline. Hints are added as comments in the code below.\n",
    "\n",
    "**NOTE:** In your Spark Declarative Pipeline we will want to add data quality expectations to document any bad data coming into the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26de3368-0214-464d-b1bb-197269b1d620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "jmEj90LTG_ji"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- You will have to modify this to create a streaming table in the pipeline\n",
    "CREATE OR REPLACE TABLE ldp_demo.ldp_schema.orders_bronze_lab4\n",
    "AS\n",
    "SELECT\n",
    "  *,\n",
    "  current_timestamp() AS ingestion_time,\n",
    "  _metadata.file_name as raw_file_name\n",
    "FROM read_files(  -- You will have to modify FROM clause to incrementally read in data\n",
    "  '/Volumes/ldp_demo/ldp_schema/raw/orders',  -- You will have to modify this path in the pipeline to your specific raw data source\n",
    "  format => 'json'\n",
    ");\n",
    "\n",
    "-- Display table\n",
    "SELECT *\n",
    "FROM ldp_demo.ldp_schema.orders_bronze_lab4;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "192d49b6-c6fe-4e95-967f-bb51e8d722fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-b1up_i4G_jl"
   },
   "source": [
    "## C. TO DO: Create the Lakeflow Spark Declarative Pipeline (Steps)\n",
    "\n",
    "After you have explored the traditional ETL code to create the tables and views, it's time to modify that syntax to declarative SQL for your new pipeline.\n",
    "\n",
    "You will have to complete the following:\n",
    "\n",
    "1. Create a Spark Declarative Pipeline and name it **Lab4 - firstname pipeline project**.\n",
    "\n",
    "    - Select your **ldp_demo** catalog  \n",
    "    - Select the **ldp_schema** schema  \n",
    "    - Select the **Start with sample code in SQL** language  \n",
    "\n",
    "2. Migrate the ETL code (shown above) into one or more files and folders to organize your pipeline.\n",
    "\n",
    "3. Modify the code to create streaming tables and materialized views with data quality expectations.\n",
    "\n",
    "4. Configure the pipeline settings:\n",
    "   - Set the **source** configuration parameter to `/Volumes/ldp_demo/ldp_schema/raw`\n",
    "\n",
    "5. Run the pipeline and validate the results.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Lab_Create_a_Pipeline",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
