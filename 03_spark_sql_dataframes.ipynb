{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6814481-5fb1-4173-8073-fdd8433af62b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Y84mq7X_Z5FP"
   },
   "source": [
    "# Module 03 - Spark SQL and DataFrames in Databricks\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module focuses on Spark SQL and advanced DataFrame operations in Databricks. Since you already know PySpark, we'll focus on Databricks-specific features and optimizations.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will understand:\n",
    "- Spark SQL in Databricks environment\n",
    "- Working with temporary views and global views\n",
    "- Advanced DataFrame operations\n",
    "- Window functions and aggregations\n",
    "- Performance optimization techniques\n",
    "- Integration between SQL and Python cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8066e4bd-58cc-4185-b43c-2acf83cf3b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "grSZYuH2Z5FT"
   },
   "source": [
    "## Creating Sample Datasets\n",
    "\n",
    "Let's create comprehensive sample datasets for our demonstrations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b05a3ad-baee-40e7-a3db-5dee7409506e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "XjzBIzhbZ5FU",
    "outputId": "5e6f2cf8-126d-44f3-b9be-39a94231fa83"
   },
   "outputs": [],
   "source": [
    "# Create sample datasets\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col, lit, rand, when\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Employees dataset\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 75000, \"2020-01-15\"),\n",
    "    (2, \"Bob\", \"Sales\", 65000, \"2019-03-20\"),\n",
    "    (3, \"Charlie\", \"Engineering\", 80000, \"2018-06-10\"),\n",
    "    (4, \"Diana\", \"Marketing\", 60000, \"2021-02-05\"),\n",
    "    (5, \"Eve\", \"Sales\", 70000, \"2020-11-12\"),\n",
    "    (6, \"Frank\", \"Engineering\", 85000, \"2017-09-01\"),\n",
    "    (7, \"Grace\", \"HR\", 55000, \"2022-01-10\"),\n",
    "    (8, \"Henry\", \"Engineering\", 90000, \"2016-04-15\"),\n",
    "]\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "employees_df = spark.createDataFrame(employees_data, employees_schema)\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "print(\"Employees DataFrame:\")\n",
    "employees_df.show()\n",
    "\n",
    "# Sales dataset\n",
    "sales_data = [\n",
    "    (\"2024-01-01\", \"Product A\", 100.0, 10, 1),\n",
    "    (\"2024-01-02\", \"Product B\", 150.0, 15, 2),\n",
    "    (\"2024-01-03\", \"Product A\", 120.0, 12, 1),\n",
    "    (\"2024-01-04\", \"Product C\", 200.0, 20, 3),\n",
    "    (\"2024-01-05\", \"Product B\", 180.0, 18, 2),\n",
    "    (\"2024-01-06\", \"Product A\", 110.0, 11, 1),\n",
    "    (\"2024-01-07\", \"Product C\", 220.0, 22, 3),\n",
    "    (\"2024-01-08\", \"Product B\", 160.0, 16, 2),\n",
    "]\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"employee_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "print(\"\\nSales DataFrame:\")\n",
    "sales_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaa6b8ba-dfe1-4d8a-8cde-dee4c70ae76c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3R6BvKLrZ5FV"
   },
   "source": [
    "## Working with Temporary Views\n",
    "\n",
    "Temporary views allow you to share DataFrames between Python and SQL cells. They exist only for the current Spark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6399574-9100-4466-bc1c-19cb37aafbe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FrTldac_Z5FV",
    "outputId": "45a8e102-3605-40c3-878b-982bc60d57b4"
   },
   "outputs": [],
   "source": [
    "# Create a temporary view from DataFrame\n",
    "employees_df.createOrReplaceTempView(\"employees_view\")\n",
    "\n",
    "# Now you can query it in SQL cells\n",
    "print(\"Temporary view 'employees_view' created\")\n",
    "print(\"You can now use it in SQL cells with: SELECT * FROM employees_view\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c922d3da-1d35-40a2-950b-ef0d691fdebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xwj9fbdfZ5FV",
    "outputId": "e8e99be1-6d63-48c3-e70f-4a8a73e4247a",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query the temporary view using SQL\n",
    "SELECT\n",
    "    department,\n",
    "    COUNT(*) as employee_count,\n",
    "    AVG(salary) as avg_salary,\n",
    "    MAX(salary) as max_salary,\n",
    "    MIN(salary) as min_salary\n",
    "FROM employees_view\n",
    "GROUP BY department\n",
    "ORDER BY avg_salary DESC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50ce1d59-a927-4200-acc1-9f2ca6fdb471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tC2viwn6Z5FW"
   },
   "source": [
    "## Global Temporary Views\n",
    "\n",
    "Global temporary views are accessible across all Spark sessions in the same cluster. They are stored in the `global_temp` database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "193474c3-cf19-49d3-9007-3b3ed0b2fbb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Ok_N_ynZZ5FW",
    "outputId": "9fd354a4-cc17-41b3-aa32-56e1c7742e6d"
   },
   "outputs": [],
   "source": [
    "# Create a global temporary view\n",
    "employees_df.createOrReplaceGlobalTempView(\"global_employees\")\n",
    "\n",
    "print(\"Global temporary view 'global_employees' created\")\n",
    "print(\"Access it in SQL with: SELECT * FROM global_temp.global_employees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c13f11b-f164-448b-a723-296e7a675876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "SbK8f2bHZ5FW",
    "outputId": "fb98498d-5b77-47b8-9395-5e6628845305",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query global temporary view\n",
    "SELECT * FROM global_temp.global_employees\n",
    "LIMIT 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e7660a-61ad-4d73-8eef-efd12f18613b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3c3PA_bHZ5FX"
   },
   "source": [
    "## Advanced SQL Queries\n",
    "\n",
    "Let's explore advanced SQL features in Databricks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710c79b8-9c18-4d28-a789-23adbe05774d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "A638DihLZ5FX",
    "outputId": "56778ae9-9186-4ef3-e073-dcfd9c03d2df",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- JOIN operations\n",
    "SELECT\n",
    "    e.name,\n",
    "    e.department,\n",
    "    e.salary,\n",
    "    s.product,\n",
    "    s.amount,\n",
    "    s.sale_date\n",
    "FROM employees e\n",
    "INNER JOIN sales s ON e.employee_id = s.employee_id\n",
    "ORDER BY s.amount DESC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2b8706-0a89-40bc-b670-7618d01f5621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "avJHw8toZ5FX",
    "outputId": "851a264f-2e9d-47d1-8539-3c5fb51c735d",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- LEFT JOIN with aggregation\n",
    "SELECT\n",
    "    e.department,\n",
    "    COUNT(DISTINCT e.employee_id) as total_employees,\n",
    "    COUNT(s.sale_date) as total_sales,\n",
    "    COALESCE(SUM(s.amount), 0) as total_revenue,\n",
    "    COALESCE(AVG(s.amount), 0) as avg_sale_amount\n",
    "FROM employees e\n",
    "LEFT JOIN sales s ON e.employee_id = s.employee_id\n",
    "GROUP BY e.department\n",
    "ORDER BY total_revenue DESC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "167323be-4eb0-4e0b-812b-c1495f1b0ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "45ZedFauZ5FX"
   },
   "source": [
    "## Window Functions\n",
    "\n",
    "Window functions are powerful for analytical queries. They allow you to perform calculations across rows related to the current row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3df0a53c-b72f-4220-85eb-4b37878e0020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nGSTYsBxZ5FX",
    "outputId": "236ef95d-3640-4880-f7e7-dcba1ead6396",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Window functions: ROW_NUMBER, RANK, DENSE_RANK\n",
    "SELECT\n",
    "    name,\n",
    "    department,\n",
    "    salary,\n",
    "    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as row_num,\n",
    "    RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank_salary,\n",
    "    DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dense_rank_salary,\n",
    "    LAG(salary, 1) OVER (PARTITION BY department ORDER BY salary DESC) as prev_salary,\n",
    "    LEAD(salary, 1) OVER (PARTITION BY department ORDER BY salary DESC) as next_salary\n",
    "FROM employees\n",
    "ORDER BY department, salary DESC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4cbad93-9121-4693-971d-16af74d55d8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "eyQJ4vKvZ5FY",
    "outputId": "54faabf9-a7b1-4637-f91f-fdd0b6aeda69",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Window functions: Running totals and averages\n",
    "SELECT\n",
    "    sale_date,\n",
    "    product,\n",
    "    amount,\n",
    "    SUM(amount) OVER (ORDER BY sale_date) as running_total,\n",
    "    AVG(amount) OVER (PARTITION BY product ORDER BY sale_date\n",
    "                      ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as moving_avg_3days,\n",
    "    SUM(amount) OVER (PARTITION BY product ORDER BY sale_date) as product_running_total\n",
    "FROM sales\n",
    "ORDER BY sale_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f594dda9-2363-463a-8df4-9f5ecb5b5fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8ARCIKt4Z5FY"
   },
   "source": [
    "## Advanced DataFrame Operations\n",
    "\n",
    "Now let's explore advanced DataFrame operations in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e9c720-9e96-4eaa-bf92-8b9344b467eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "cx5oeo9BZ5FY",
    "outputId": "679cd551-a5aa-48be-9c5a-dc4dc6a68e8b"
   },
   "outputs": [],
   "source": [
    "# Window functions in PySpark\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, sum as spark_sum, avg as spark_avg\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Apply window functions\n",
    "result_df = employees_df.withColumn(\n",
    "    \"row_num\", row_number().over(window_spec)\n",
    ").withColumn(\n",
    "    \"rank_salary\", rank().over(window_spec)\n",
    ").withColumn(\n",
    "    \"dense_rank_salary\", dense_rank().over(window_spec)\n",
    ").withColumn(\n",
    "    \"prev_salary\", lag(\"salary\", 1).over(window_spec)\n",
    ").withColumn(\n",
    "    \"next_salary\", lead(\"salary\", 1).over(window_spec)\n",
    ")\n",
    "\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a7262c-f604-4ff4-9f61-77f29eeb5724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "adTT72MsZ5FY",
    "outputId": "1ac96bb1-a76d-4fd7-d067-1f810e1add82"
   },
   "outputs": [],
   "source": [
    "# Complex aggregations\n",
    "from pyspark.sql.functions import count, countDistinct, collect_list, collect_set\n",
    "\n",
    "agg_result = employees_df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"total_employees\"),\n",
    "    countDistinct(\"employee_id\").alias(\"unique_employees\"),\n",
    "    spark_avg(\"salary\").alias(\"avg_salary\"),\n",
    "    spark_sum(\"salary\").alias(\"total_salary\"),\n",
    "    collect_list(\"name\").alias(\"employee_names\"),\n",
    "    collect_set(\"name\").alias(\"unique_names\")\n",
    ")\n",
    "\n",
    "display(agg_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27cc62ab-f8cc-4f50-8db6-5593d84ad4f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EhGLuKHJZ5FY",
    "outputId": "d989ad52-7301-457f-964c-37795db2da58"
   },
   "outputs": [],
   "source": [
    "# Joins in PySpark\n",
    "joined_df = employees_df.join(\n",
    "    sales_df,\n",
    "    employees_df.employee_id == sales_df.employee_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    employees_df[\"name\"],\n",
    "    employees_df[\"department\"],\n",
    "    sales_df[\"product\"],\n",
    "    sales_df[\"amount\"],\n",
    "    sales_df[\"sale_date\"]\n",
    ")\n",
    "\n",
    "display(joined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b448a19f-bdad-4132-ad41-154ba5042e9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "w8mZ0eBxZ5FY",
    "outputId": "a80a8370-0c1c-4c78-f5aa-0e9e0e9d8523"
   },
   "outputs": [],
   "source": [
    "# Pivot operations\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "pivot_df = sales_df.groupBy(\"sale_date\").pivot(\"product\").agg(\n",
    "    spark_sum(\"amount\").alias(\"total_amount\")\n",
    ").na.fill(0)\n",
    "\n",
    "display(pivot_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d7871a-baec-4d0c-befe-5e1a6d86e9cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hD3ag-r-Z5FZ"
   },
   "source": [
    "## User-Defined Functions (UDFs)\n",
    "\n",
    "UDFs allow you to extend Spark SQL with custom functions. However, use them sparingly as they can impact performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1462666b-3ff8-4a79-9ebb-00709844a73e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "SYDrzDyJZ5FZ",
    "outputId": "fe27eb03-b2cf-4ea8-9f68-d2aa85a75ba8"
   },
   "outputs": [],
   "source": [
    "# Register UDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Simple UDF\n",
    "def categorize_salary(salary):\n",
    "    if salary >= 80000:\n",
    "        return \"High\"\n",
    "    elif salary >= 65000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "salary_category_udf = udf(categorize_salary, StringType())\n",
    "\n",
    "# Use UDF\n",
    "result_df = employees_df.withColumn(\n",
    "    \"salary_category\",\n",
    "    salary_category_udf(col(\"salary\"))\n",
    ")\n",
    "\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ee9019-a9e5-4a32-9370-cdb9aebe959f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fdsQqPVIZ5FZ",
    "outputId": "38aef615-4ff3-41d7-b547-fbcad5ee9990"
   },
   "outputs": [],
   "source": [
    "# Register UDF for SQL use\n",
    "spark.udf.register(\"categorize_salary\", categorize_salary, StringType())\n",
    "\n",
    "# Now use it in SQL\n",
    "print(\"UDF registered. You can now use it in SQL cells:\")\n",
    "print(\"SELECT name, salary, categorize_salary(salary) as category FROM employees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df40c37a-2895-4898-8925-84e866a08d88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9HLPHyIXZ5FZ",
    "outputId": "4f52199a-4802-4721-a5bc-adc1da07797f",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Use the registered UDF\n",
    "SELECT\n",
    "    name,\n",
    "    salary,\n",
    "    categorize_salary(salary) as salary_category\n",
    "FROM employees\n",
    "ORDER BY salary DESC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1baa7dc-523f-445c-9371-0c1a1201b8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-AX-xJEuZ5FZ"
   },
   "source": [
    "## Performance Optimization Techniques\n",
    "\n",
    "### 1. Caching and Persistence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e244201-c0c0-4ae8-a1fb-2b1b8ca37b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Uand2cfWZ5FZ",
    "outputId": "6010cbef-8d37-403d-f936-9d7c4de7f647"
   },
   "outputs": [],
   "source": [
    "# Create a complex DataFrame that will be reused\n",
    "complex_df = employees_df.join(\n",
    "    sales_df,\n",
    "    employees_df.employee_id == sales_df.employee_id,\n",
    "    \"inner\"\n",
    ").groupBy(\"department\", \"product\").agg(\n",
    "    spark_sum(\"amount\").alias(\"total_amount\"),\n",
    "    spark_avg(\"amount\").alias(\"avg_amount\"),\n",
    "    count(\"*\").alias(\"transaction_count\")\n",
    ")\n",
    "\n",
    "# Cache the DataFrame\n",
    "complex_df.cache()\n",
    "\n",
    "# First action - will compute and cache\n",
    "print(f\"Cached DataFrame count: {complex_df.count()}\")\n",
    "\n",
    "# Subsequent actions - will use cache\n",
    "print(\"\\nUsing cached DataFrame:\")\n",
    "display(complex_df.filter(col(\"total_amount\") > 150))\n",
    "\n",
    "# Unpersist when done\n",
    "complex_df.unpersist()\n",
    "print(\"\\nDataFrame unpersisted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b37ccc49-f3e2-4568-8f04-5983e49f8e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wkEVTbjQZ5Fa"
   },
   "source": [
    "### 2. Broadcast Joins\n",
    "\n",
    "For small lookup tables, use broadcast joins to improve performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237dc486-b865-478c-91ad-ad457c4acbd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "gwN5Rd8hZ5Fa",
    "outputId": "143367fd-2834-419d-9fc3-f5cbd74890c0"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Create a small lookup table\n",
    "departments_df = spark.createDataFrame([\n",
    "    (\"Engineering\", \"Tech\"),\n",
    "    (\"Sales\", \"Business\"),\n",
    "    (\"Marketing\", \"Business\"),\n",
    "    (\"HR\", \"Support\")\n",
    "], [\"department\", \"category\"])\n",
    "\n",
    "# Use broadcast join for small table\n",
    "joined_with_broadcast = sales_df.join(\n",
    "    broadcast(departments_df),\n",
    "    sales_df.product == departments_df.department,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "display(joined_with_broadcast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7f4129b-8c7c-4b48-ae87-3a87cf12a030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tGrgIp8CZ5Fa"
   },
   "source": [
    "### 3. Partitioning and Coalescing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9247585c-58cb-4a42-a699-e099ce887466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hJppzKdrZ5Fa",
    "outputId": "48b1633e-4785-4520-cac5-8bb3a4020803"
   },
   "outputs": [],
   "source": [
    "# Check current partitions\n",
    "print(f\"Number of partitions: {employees_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition\n",
    "repartitioned_df = employees_df.repartition(4)\n",
    "print(f\"After repartition: {repartitioned_df.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Coalesce (reduces partitions)\n",
    "coalesced_df = repartitioned_df.coalesce(2)\n",
    "print(f\"After coalesce: {coalesced_df.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Repartition by column (useful for joins)\n",
    "repartitioned_by_dept = employees_df.repartition(\"department\")\n",
    "print(f\"Repartitioned by department: {repartitioned_by_dept.rdd.getNumPartitions()} partitions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa3edf5f-e846-402b-9e2e-ce68fec20629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aZb6ASRTZ5Fa"
   },
   "source": [
    "## Converting Between SQL and DataFrames\n",
    "\n",
    "One of Databricks' strengths is seamless integration between SQL and Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e7c72c-410e-47a8-9e80-a6b83f70734d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mZJWc0EvZ5Fa",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a view from SQL query\n",
    "CREATE OR REPLACE TEMP VIEW department_summary AS\n",
    "SELECT\n",
    "    e.department,\n",
    "    COUNT(DISTINCT e.employee_id) as employee_count,\n",
    "    AVG(e.salary) as avg_salary,\n",
    "    SUM(s.amount) as total_sales\n",
    "FROM employees e\n",
    "LEFT JOIN sales s ON e.employee_id = s.employee_id\n",
    "GROUP BY e.department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc01efc-6670-43e8-a0ec-250d3edd5e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tQOHZNaaZ5Fa",
    "outputId": "fe215202-e710-4b35-fd86-f006f449cdb8"
   },
   "outputs": [],
   "source": [
    "# Use the SQL view in Python\n",
    "dept_summary_df = spark.table(\"department_summary\")\n",
    "display(dept_summary_df)\n",
    "\n",
    "# Now you can use DataFrame operations\n",
    "filtered_df = dept_summary_df.filter(col(\"total_sales\") > 0)\n",
    "display(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06081658-aa85-48ee-8bdc-6c81d6591176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "i1p5OBELZ5Fa",
    "outputId": "0621b28e-051b-457b-f308-a98156229aea"
   },
   "outputs": [],
   "source": [
    "# Execute SQL from Python and get result as DataFrame\n",
    "sql_query = \"\"\"\n",
    "SELECT\n",
    "    e.name,\n",
    "    e.department,\n",
    "    COUNT(s.sale_date) as sales_count,\n",
    "    SUM(s.amount) as total_sales\n",
    "FROM employees e\n",
    "LEFT JOIN sales s ON e.employee_id = s.employee_id\n",
    "GROUP BY e.name, e.department\n",
    "ORDER BY total_sales DESC\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(sql_query)\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75ace8e2-27d6-47a6-aa3c-e680612c3bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aWfvguToZ5Fa"
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Use SQL for complex queries** - Often more readable for complex joins and aggregations\n",
    "2. **Use DataFrames for programmatic logic** - Better for conditional logic and loops\n",
    "3. **Cache frequently used DataFrames** - But remember to unpersist when done\n",
    "4. **Use broadcast joins for small tables** - Improves join performance\n",
    "5. **Partition wisely** - Too many partitions can hurt performance\n",
    "6. **Avoid UDFs when possible** - Use built-in functions for better performance\n",
    "7. **Use temporary views** - Share data between SQL and Python cells\n",
    "8. **Leverage display()** - Better visualization than show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adda37bc-d9cd-4374-8037-7a00604ab9b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FzLgK67IZ5Fb"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "✅ **Temporary and Global Views** - Sharing data between SQL and Python\n",
    "\n",
    "✅ **Advanced SQL** - JOINs, aggregations, and complex queries\n",
    "\n",
    "✅ **Window Functions** - ROW_NUMBER, RANK, running totals, and moving averages\n",
    "\n",
    "✅ **Advanced DataFrame Operations** - Window functions, pivots, and complex aggregations\n",
    "\n",
    "✅ **UDFs** - Creating and using user-defined functions\n",
    "\n",
    "✅ **Performance Optimization** - Caching, broadcast joins, and partitioning\n",
    "\n",
    "✅ **SQL-DataFrame Integration** - Seamless switching between SQL and Python\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next module, we'll explore:\n",
    "- Delta Lake: ACID transactions and time travel\n",
    "- Unity Catalog: Data governance and cataloging\n",
    "- Jobs: Scheduling and automation\n",
    "- Advanced Databricks features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02a00ce2-b16e-4de7-80bd-1a6a07b3fd19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0YIteVefZ5Fb"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Try these exercises to practice:\n",
    "\n",
    "1. Create a complex SQL query with multiple JOINs and window functions\n",
    "2. Convert the SQL query result to a DataFrame and apply additional transformations\n",
    "3. Create a UDF and use it in both Python and SQL\n",
    "5. Use broadcast join for a small lookup table\n",
    "7. Use window functions to calculate running totals and moving averages\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7184277210543163,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_spark_sql_dataframes",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
