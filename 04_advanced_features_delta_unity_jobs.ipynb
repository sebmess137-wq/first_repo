{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeaaa17f-376e-413e-a6c2-eb47485417c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yVb_lAEIh60U"
   },
   "source": [
    "# Module 04 - Advanced Features: Delta Lake, Unity Catalog, and Jobs\n",
    "\n",
    "## Overview\n",
    "\n",
    "This module covers advanced Databricks features including Delta Lake for ACID transactions, Unity Catalog for data governance, and Jobs for automation.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will understand:\n",
    "- Delta Lake: ACID transactions, time travel, and schema evolution\n",
    "- Unity Catalog: Data governance and cataloging\n",
    "- Jobs: Scheduling and automating notebook execution\n",
    "- Workflows: Orchestrating multiple tasks\n",
    "- Best practices for production workloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de3857be-79f4-4c2a-997d-e1445f67987d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zKqqCp6wh60Y"
   },
   "source": [
    "## Introduction to Delta Lake\n",
    "\n",
    "Delta Lake is an open-source storage layer that brings ACID transactions to data lakes. It's built on top of Parquet and provides:\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **ACID Transactions**: Ensures data consistency\n",
    "2. **Time Travel**: Query historical versions of data\n",
    "3. **Schema Enforcement**: Prevents bad data from being written\n",
    "4. **Schema Evolution**: Allows schema changes over time\n",
    "5. **Upserts**: Update and insert operations (MERGE)\n",
    "6. **Optimized Performance**: Better query performance than Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7829aad2-5cb4-4e68-8d92-e30f28ac455e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Uxv7ykSZh60Z",
    "outputId": "039e01f4-4131-4e10-c132-2b87e77e56f5"
   },
   "outputs": [],
   "source": [
    "# Create sample data for Delta Lake demonstrations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "# Initial dataset\n",
    "data = [\n",
    "    (1, \"Product A\", 100.0, 10),\n",
    "    (2, \"Product B\", 150.0, 15),\n",
    "    (3, \"Product C\", 200.0, 20),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "print(\"Initial DataFrame:\")\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93af7f61-a9d7-4612-9ddb-70da0f48a321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FjQ4t7DLh60b"
   },
   "source": [
    "## Creating Delta Tables\n",
    "\n",
    "Delta tables are created by writing DataFrames in Delta format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef5affbd-153f-4229-9a67-6f210f964a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8t2DcFDLh60c",
    "outputId": "552c729e-04f7-4f99-a7fe-ab1026dc0366"
   },
   "outputs": [],
   "source": [
    "# Create a Delta table\n",
    "delta_path = \"/Volumes/workspace/default/training_volume/products_delta\"\n",
    "\n",
    "# Write as Delta\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "print(f\"Delta table created at: {delta_path}\")\n",
    "\n",
    "# Read Delta table\n",
    "delta_df = spark.read.format(\"delta\").load(delta_path)\n",
    "print(\"\\nReading Delta table:\")\n",
    "delta_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a8b0750-889a-4aa8-a6cb-35ec05a41678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Q46f5uSEh60d"
   },
   "source": [
    "## Delta Table Operations\n",
    "\n",
    "### 1. Append Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d29bef0-fcf7-436c-91a2-ad5a7d670300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hUQob8vah60d",
    "outputId": "5d43ecf6-ea2b-453c-dc14-d08f8e5508b1"
   },
   "outputs": [],
   "source": [
    "# Append new data to Delta table\n",
    "new_data = [\n",
    "    (4, \"Product D\", 250.0, 25),\n",
    "    (5, \"Product E\", 300.0, 30),\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data, schema)\n",
    "new_df.write.format(\"delta\").mode(\"append\").save(delta_path)\n",
    "\n",
    "# Read updated table\n",
    "updated_df = spark.read.format(\"delta\").load(delta_path)\n",
    "print(\"After appending:\")\n",
    "updated_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bab9af13-2062-4bb1-912b-92e3cd28a75e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "YJ-gKojrh60e"
   },
   "source": [
    "### 2. Update Operations (MERGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1e6eaf-f125-4dc3-937d-673748f96e53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "beL_i2juh60e",
    "outputId": "977c2281-326e-4954-cc53-269b7ee465a5"
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Create updates DataFrame and add 'category' column with nulls to match Delta table schema\n",
    "updates = [\n",
    "    (1, \"Product A Updated\", 110.0, 12),  # Update existing\n",
    "    (6, \"Product F\", 350.0, 35),          # New record\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(updates, schema).withColumn(\"category\", lit(None).cast(\"string\"))\n",
    "\n",
    "# Perform MERGE operation\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    updates_df.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "print(\"MERGE operation completed\")\n",
    "print(\"\\nUpdated Delta table:\")\n",
    "spark.read.format(\"delta\").load(delta_path).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "097377de-ddf0-4ed8-b4ec-db6cc7512322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "t-eT7065h60f"
   },
   "source": [
    "### 3. Time Travel\n",
    "\n",
    "Delta Lake maintains a history of all changes, allowing you to query previous versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be3b3bf2-9015-4a0f-97a6-13d4bbc9b594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Rmc0jIpXh60f",
    "outputId": "7b1668d3-49f2-42dd-8d09-91585d0a2e8f"
   },
   "outputs": [],
   "source": [
    "# Get history of Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "history = delta_table.history()\n",
    "\n",
    "print(\"Delta table history:\")\n",
    "display(history)\n",
    "\n",
    "# Query a specific version\n",
    "print(\"\\nQuerying version 0 (initial version):\")\n",
    "version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\n",
    "display(version_0)\n",
    "\n",
    "# Query by timestamp (if you know the timestamp)\n",
    "# timestamp_df = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01 00:00:00\").load(delta_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "786171ec-cb90-4390-bc12-855520035c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9m0VWTQ-h60f"
   },
   "source": [
    "### 4. Schema Evolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa3da42-4c43-48d3-ba54-715743f9efd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hg9eQvMMh60f",
    "outputId": "e934daac-376d-4a03-f655-8a3ca26f8949"
   },
   "outputs": [],
   "source": [
    "# Add a new column to the schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# New data with additional column\n",
    "new_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True)  # New column\n",
    "])\n",
    "\n",
    "new_data_with_category = [\n",
    "    (7, \"Product G\", 400.0, 40, \"Electronics\"),\n",
    "    (8, \"Product H\", 450.0, 45, \"Electronics\"),\n",
    "]\n",
    "\n",
    "new_df_with_category = spark.createDataFrame(new_data_with_category, new_schema)\n",
    "\n",
    "# Write with mergeSchema option to evolve schema\n",
    "new_df_with_category.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(delta_path)\n",
    "\n",
    "print(\"Schema evolved - new column 'category' added\")\n",
    "spark.read.format(\"delta\").load(delta_path).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "375eb09b-2edf-48ca-98ae-4b258cc10c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "P9hRf7Ylh60f"
   },
   "source": [
    "## Delta Lake Best Practices\n",
    "\n",
    "1. **Use Delta for all production data** - Better performance and reliability\n",
    "2. **Partition large tables** - Improves query performance\n",
    "3. **Compact small files** - Use OPTIMIZE to merge small files\n",
    "4. **Vacuum old versions** - Clean up old data files (be careful with time travel)\n",
    "5. **Use Z-ordering** - For better query performance on specific columns\n",
    "6. **Enable schema enforcement** - Prevent bad data\n",
    "7. **Use mergeSchema carefully** - Understand the implications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c129f397-c9d8-462e-99f2-4fd986e2bbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "RboBaXjgh60f",
    "outputId": "f9ff9430-4d32-4d4d-961e-0176e261363f"
   },
   "outputs": [],
   "source": [
    "# Optimize Delta table (compact small files)\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "delta_table.optimize().executeCompaction()\n",
    "\n",
    "print(\"Delta table optimized\")\n",
    "\n",
    "# Z-order by specific column for better query performance\n",
    "# delta_table.optimize().executeZOrderBy(\"product\")\n",
    "\n",
    "# Vacuum old files (removes files older than retention period)\n",
    "# delta_table.vacuum(retentionHours=168)  # 7 days\n",
    "# print(\"Vacuum completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15fb76d0-fc2a-422d-b9df-56dbc87463b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "a6xfU5-ch60g"
   },
   "source": [
    "## Unity Catalog (Overview)\n",
    "\n",
    "Unity Catalog is Databricks' unified governance solution for data and AI. It provides:\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Centralized Metadata**: Single source of truth for all data assets\n",
    "2. **Fine-grained Access Control**: Column and row-level security\n",
    "3. **Data Lineage**: Track data flow and dependencies\n",
    "4. **Audit Logging**: Monitor data access and changes\n",
    "5. **Cross-cloud Support**: Works across AWS, Azure, and GCP\n",
    "\n",
    "### Note for Free Tier\n",
    "\n",
    "Unity Catalog may have limited features in the free tier. Check your Databricks edition for availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6cdcc6d-4a12-4ff5-a953-45b2faeec749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2KVRa999h60g",
    "outputId": "68e334cb-6226-412e-a59c-73c01349b4ed"
   },
   "outputs": [],
   "source": [
    "# Unity Catalog concepts (if available in your workspace)\n",
    "# Unity Catalog uses a three-level namespace: catalog.schema.table\n",
    "\n",
    "# Example: Query from Unity Catalog\n",
    "# df = spark.table(\"main.default.products\")\n",
    "\n",
    "# Create table in Unity Catalog (if you have permissions)\n",
    "# df.write.saveAsTable(\"main.default.products\")\n",
    "\n",
    "print(\"Unity Catalog is available in Databricks workspaces with appropriate licenses.\")\n",
    "print(\"It provides centralized governance for all your data assets.\")\n",
    "print(\"\\nKey concepts:\")\n",
    "print(\"- Catalog: Top-level container (e.g., 'main')\")\n",
    "print(\"- Schema/Database: Second-level container\")\n",
    "print(\"- Table: Actual data table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22b1674f-7e7b-4a77-8eba-0ad4dfb2380f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8UW8EPKJh60g"
   },
   "source": [
    "## Working with Managed Tables\n",
    "\n",
    "Managed tables are tables where Databricks manages both the data and metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f691da9d-5bf4-4d5b-9de5-a0aaba125d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8R0ShwxCh60g",
    "outputId": "3ba29c84-081d-4db1-d71b-70ed9c81bd17"
   },
   "outputs": [],
   "source": [
    "# Create a managed table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"products_managed\")\n",
    "\n",
    "print(\"Managed table 'products_managed' created\")\n",
    "print(\"You can query it with: SELECT * FROM products_managed\")\n",
    "\n",
    "# Query the managed table\n",
    "spark.table(\"products_managed\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b140f59-f9cb-4e5f-a636-0e9280d217c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1tEUfT35h60h",
    "outputId": "5027db70-9bf1-4c1d-b7fd-d5a0b700ed7a",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query managed table using SQL\n",
    "SELECT * FROM products_managed\n",
    "ORDER BY price DESC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ebf5778-d0ee-4695-9ada-9acc1c9e8a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5icb5pdOh60h"
   },
   "source": [
    "## Databricks Jobs\n",
    "\n",
    "Jobs allow you to run notebooks or scripts on a schedule or trigger. They're essential for production workloads.\n",
    "\n",
    "### Job Types\n",
    "\n",
    "1. **Notebook Jobs**: Run Databricks notebooks\n",
    "2. **Python Scripts**: Run Python files\n",
    "3. **JAR Jobs**: Run JAR files\n",
    "4. **Spark Submit**: Run Spark applications\n",
    "\n",
    "### Job Features\n",
    "\n",
    "- **Scheduling**: Cron-based scheduling\n",
    "- **Retries**: Automatic retry on failure\n",
    "- **Notifications**: Email/Slack alerts\n",
    "- **Job Clusters**: Automatic cluster management\n",
    "- **Dependencies**: Chain multiple jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6cc1b5-892c-46be-8860-14fc29c3fb9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "g8n-tvXUh60h",
    "outputId": "5a29a44d-920f-4d6c-d8e9-720edce2dbc8"
   },
   "outputs": [],
   "source": [
    "# Jobs are typically created via the Databricks UI or REST API\n",
    "# Here's how to create a job programmatically using Databricks API\n",
    "\n",
    "print(\"Creating Jobs:\")\n",
    "print(\"\\n1. Via UI:\")\n",
    "print(\"   - Go to Workflows > Jobs\")\n",
    "print(\"   - Click 'Create Job'\")\n",
    "print(\"   - Add tasks (notebooks, scripts, etc.)\")\n",
    "print(\"   - Configure schedule and cluster\")\n",
    "print(\"   - Set up notifications\")\n",
    "\n",
    "print(\"\\n2. Via Databricks CLI:\")\n",
    "print(\"   databricks jobs create --json-file job_config.json\")\n",
    "\n",
    "print(\"\\n3. Via REST API:\")\n",
    "print(\"   POST /api/2.1/jobs/create\")\n",
    "\n",
    "print(\"\\nExample job configuration:\")\n",
    "job_config_example = {\n",
    "    \"name\": \"Daily ETL Job\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"extract_data\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Users/your_email@domain.com/extract_data\"\n",
    "            },\n",
    "            \"existing_cluster_id\": \"your-cluster-id\"\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"transform_data\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Users/your_email@domain.com/transform_data\"\n",
    "            },\n",
    "            \"depends_on\": [{\"task_key\": \"extract_data\"}],\n",
    "            \"existing_cluster_id\": \"your-cluster-id\"\n",
    "        }\n",
    "    ],\n",
    "    \"schedule\": {\n",
    "        \"quartz_cron_expression\": \"0 0 2 * * ?\",  # Daily at 2 AM\n",
    "        \"timezone_id\": \"America/New_York\"\n",
    "    },\n",
    "    \"email_notifications\": {\n",
    "        \"on_success\": [\"your-email@domain.com\"],\n",
    "        \"on_failure\": [\"your-email@domain.com\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nJob config structure:\")\n",
    "for key, value in job_config_example.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ead39096-2f9c-4977-8b39-62d4cdb92d9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "d4CVfx5Vh60h"
   },
   "source": [
    "## Parameterizing Notebooks for Jobs\n",
    "\n",
    "Notebooks can accept parameters when run as jobs, making them reusable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7213830-1597-4ad2-8f62-f73844fbb6d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "H9fwxVf_h60h",
    "outputId": "0da4debe-8b8f-47bd-cff3-e267b8c45695"
   },
   "outputs": [],
   "source": [
    "# Using widgets for parameters\n",
    "dbutils.widgets.text(\"input_path\", \"/tmp/input\", \"Input Path\")\n",
    "dbutils.widgets.text(\"output_path\", \"/tmp/output\", \"Output Path\")\n",
    "dbutils.widgets.dropdown(\"mode\", \"overwrite\", [\"overwrite\", \"append\"], \"Write Mode\")\n",
    "\n",
    "# Get parameter values\n",
    "input_path = dbutils.widgets.get(\"input_path\")\n",
    "output_path = dbutils.widgets.get(\"output_path\")\n",
    "mode = dbutils.widgets.get(\"mode\")\n",
    "\n",
    "print(f\"Input Path: {input_path}\")\n",
    "print(f\"Output Path: {output_path}\")\n",
    "print(f\"Mode: {mode}\")\n",
    "\n",
    "# Use parameters in your code\n",
    "# df = spark.read.parquet(input_path)\n",
    "# df.write.mode(mode).parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "161ffef3-8f29-4543-84ab-da688d8f39dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "oijf7Enzh60h"
   },
   "source": [
    "## Running Notebooks from Other Notebooks\n",
    "\n",
    "You can orchestrate workflows by running notebooks from other notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d156ba5-c679-447f-8f74-f6bf9c34bbec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FkWYYGvqh60h"
   },
   "outputs": [],
   "source": [
    "# Run another notebook\n",
    "# result = dbutils.notebook.run(\n",
    "#     \"/path/to/other/notebook\",\n",
    "#     timeout_seconds=300,\n",
    "#     arguments={\n",
    "#         \"param1\": \"value1\",\n",
    "#         \"param2\": \"value2\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# print(f\"Notebook execution result: {result}\")\n",
    "\n",
    "print(\"To run a notebook from another notebook:\")\n",
    "print(\"result = dbutils.notebook.run('/path/to/notebook', timeout_seconds=300)\")\n",
    "print(\"\\nThis is useful for:\")\n",
    "print(\"- Orchestrating multi-step workflows\")\n",
    "print(\"- Reusing common logic\")\n",
    "print(\"- Building modular data pipelines\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5596ad5b-5353-4d15-92c2-e9742f1d302f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8vwCn95Jh60h"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "✅ **Delta Lake** - ACID transactions, time travel, schema evolution, and MERGE operations\n",
    "\n",
    "✅ **Unity Catalog** - Data governance and centralized metadata management\n",
    "\n",
    "✅ **Managed Tables** - Creating and managing tables in Databricks\n",
    "\n",
    "✅ **Jobs** - Scheduling and automating notebook execution\n",
    "\n",
    "✅ **Parameterization** - Making notebooks reusable with widgets\n",
    "\n",
    "✅ **Notebook Orchestration** - Running notebooks from other notebooks\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the final module, we'll explore:\n",
    "- Production best practices\n",
    "- Performance optimization\n",
    "- Monitoring and debugging\n",
    "- Real-world scenarios and case studies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fecf5e94-c12c-4919-944a-ddbbed925076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-QBAv8XWh60h"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Try these exercises to practice:\n",
    "\n",
    "1. Create a Delta table and perform INSERT, UPDATE, and DELETE operations\n",
    "2. Use time travel to query a previous version of your Delta table\n",
    "3. Evolve the schema of a Delta table by adding a new column\n",
    "4. Create a managed table and query it using SQL\n",
    "7. Use MERGE to upsert data into a Delta table\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5296116087534884,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_advanced_features_delta_unity_jobs",
   "widgets": {
    "input_path": {
     "currentValue": "/tmp/input",
     "nuid": "b12e5bd3-bd8e-44d9-a613-11c7b9a3af66",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/tmp/input",
      "label": "Input Path",
      "name": "input_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/tmp/input",
      "label": "Input Path",
      "name": "input_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "mode": {
     "currentValue": "overwrite",
     "nuid": "46732264-1100-480f-9775-b3c4e8198256",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "overwrite",
      "label": "Write Mode",
      "name": "mode",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "overwrite",
        "append"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "overwrite",
      "label": "Write Mode",
      "name": "mode",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "overwrite",
        "append"
       ]
      }
     }
    },
    "output_path": {
     "currentValue": "/tmp/output",
     "nuid": "eab52c79-9225-4166-bc3d-e5a03c355162",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/tmp/output",
      "label": "Output Path",
      "name": "output_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/tmp/output",
      "label": "Output Path",
      "name": "output_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
